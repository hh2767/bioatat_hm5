---
title: "HW5_hh2767"
author: "Haoran Hu"
date: "2018-11-30"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(faraway)
library(reshape2)
library(patchwork)
library(broom)
library(leaps)
library(caret)


theme_set(theme_bw() + theme(legend.position = "bottom"))
```

```{r}
life_exp = state.x77 %>% 
  as.tibble()
```

#Problem1.Descriptive statistics

In this part, I will use some functions that I created in previous homework to generate descriptive statistics. There seems to be no categorical variables, i.e., all variables can be treated as continuous variable.

```{r}
#A function for descirptive statistics for continuous variables. The input is a dataframe.
descriptive_statistics_cont = function(x){
  data_mean = purrr::map_df(x, mean, na.rm = TRUE)
  data_quantile = purrr::map_df(x, quantile, na.rm = TRUE)
  data_sd = purrr::map_df(x, sd, na.rm = TRUE)
  n = colSums(!is.na(x)) 
  na = colSums(is.na(x)) 
  variable_names = colnames(x)
  
 output1 = tibble(
Variables = variable_names,
Minimun = as.numeric(data_quantile[1,]),
`1st_quantile` = as.numeric(data_quantile[2,]),
Median = as.numeric(data_quantile[3,]),
Mean = as.numeric(round(data_mean[1,], 3)),
`3rd_quantile` = as.numeric(data_quantile[4,]),
Max = as.numeric(data_quantile[5,])
) 
  
 output2 = tibble(
Variables = variable_names,
Standard_deviation = as.numeric(round(data_sd, 3)),
Sample_sizes = n,
Number_of_NA = na
) 
 
 list(output1, output2)
}

#create a dunction to get mode
getmode = function(v) {
   uniqv = unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

#A function for descriptive statistics for discrete variables, the input is a vector
descriptive_statistics_disc = function(x){
  levels = unique(x) 
  n_levels = length(levels)
  levels = levels %>% tibble()
  n_total = length(x)
  if (n_levels <= 2) {
    level1 = as.numeric(levels[1,1])
    level2 = as.numeric(levels[2,1])
    n_level1 = sum(x == level1)
    n_level2 = sum(x == level2)
    tibble(`levels` = c(level1, level2),
           proportion = c(round(n_level1 / n_total, 4), round(n_level2 / n_total, 4))
           )
    
  } else {
    x_quantile = quantile(x) %>% tibble()
    x_mode = getmode(x)
    tibble(
      min = as.numeric(x_quantile[1,]),
      `1st_quantile` = as.numeric(x_quantile[2,]),
      median = as.numeric(x_quantile[3,]),
      mode = as.numeric(x_mode),
      `3rd_quantile` = as.numeric(x_quantile[4,]),
      max = as.numeric(x_quantile[5,])
    )
  }
}
```

```{r}
life_exp_descrip = descriptive_statistics_cont(life_exp)

discrip_table1 = life_exp_descrip[[1]] 

discrip_table2 = life_exp_descrip[[2]]

knitr::kable(discrip_table1)
knitr::kable(discrip_table2)
```

```{r fig.width = 10, fig.asp = .8}
life_plot = ggplot(life_exp, aes(x = `Life Exp`)) + geom_histogram(fill = "navy", alpha = 0.5) + labs(title = "Distribution of life expactancy")
population_plot = ggplot(life_exp, aes(y = Population)) + geom_boxplot(fill = "navy", alpha = 0.5) + labs(title = "Distribution of population")
income_plot =  ggplot(life_exp, aes(y = Income)) + geom_boxplot(fill = "navy", alpha = 0.5) + labs(title = "Distribution of income")
illiteracy_plot =  ggplot(life_exp, aes(y = Illiteracy)) + geom_boxplot(fill = "navy", alpha = 0.5) + labs(title = "Distribution of illiteracy rate")
murder_plot = ggplot(life_exp, aes(y = Murder)) + geom_boxplot(fill = "navy", alpha = 0.5) + labs(title = "Distribution of murder rate")
hs_grad_plot = ggplot(life_exp, aes(y = `HS Grad`)) + geom_boxplot(fill = "navy", alpha = 0.5) + labs(title = "Distribution of proportion of high school graduates")
frost_plot = ggplot(life_exp, aes(y = Frost)) + geom_boxplot(fill = "navy", alpha = 0.5) + labs(title = "Distribution of 'Frost'")
area_plot = ggplot(life_exp, aes(y = Frost)) + geom_boxplot(fill = "navy", alpha = 0.5) + labs(title = "Distribution of area")

(life_plot + population_plot) / (income_plot + illiteracy_plot) /  (murder_plot + hs_grad_plot) / (frost_plot + area_plot)

```

#Problem2.Building model

Next, I will use automatic procedures to select subsets and build several good models. The original model is:

```{r}
origin_fit <- lm(`Life Exp` ~ ., data = life_exp)
```

###Backward elimination

First, let's try backward elimination.In this part, we set$\alpha_{crit} = 0.10$.

```{r}
summary(origin_fit)
#No Area
step1 = update(origin_fit, . ~ . -Area)
summary(step1)
#No Illiteracy
step2 = update(step1, . ~ . -Illiteracy)
summary(step2)
#No Income
step3 = update(step2, . ~ . -Income)
summary(step3)
```

Therefore, using backward elimination, we selected 'Population', 'Murder', 'HS Grad' and 'Frost' as predictors. The model is given by: 

`Life Exp` ~ `Population` + `Murder` + `HS Grad` + `Frost`.

The fitted regression line is:

`Life Exp` = 71 + 0.00005`Population` - 0.3`Murder` + 0.047`Hs Grad` - 0.006`Frost`

###Forward elimination

Second, let's use forward elimination. In this part, we also set$\alpha_{crit} = 0.10$.

```{r}
# Step 1:  Fit simple linear regressions for all variables,look for the variable with lowest p-value
fit1 <- lm(`Life Exp` ~ Population, data = life_exp)
tidy(fit1)
fit2 <- lm(`Life Exp` ~ Income, data = life_exp)
tidy(fit2)
fit3 <- lm(`Life Exp` ~ Illiteracy, data = life_exp)
tidy(fit3)
fit4 <- lm(`Life Exp` ~ Murder, data = life_exp)
tidy(fit4)
fit5 <- lm(`Life Exp` ~ `HS Grad`, data = life_exp)
tidy(fit5)
fit6 <- lm(`Life Exp` ~ Frost, data = life_exp)
tidy(fit6)
fit7 <- lm(`Life Exp` ~ Area, data = life_exp)
tidy(fit7)

# Enter first the one with the lowest p-value: Murder
forward1 <- lm(`Life Exp`~ Murder, data = life_exp)
tidy(forward1)
# Step 2: Enter the one with the lowest p-value in the rest 
fit1 <- update(forward1, . ~ . +Population, data = life_exp)
tidy(fit1)
fit2 <- update(forward1, . ~ . +Income, data = life_exp)
tidy(fit2)
fit3 <- update(forward1, . ~ . +Illiteracy, data = life_exp)
tidy(fit3)
fit4 <- update(forward1, . ~ . +`HS Grad`, data = life_exp)
tidy(fit4)
fit5 <- update(forward1, . ~ . + Frost, data = life_exp)
tidy(fit5)
fit6 <- update(forward1, . ~ . +Area, data = life_exp)
tidy(fit6)
# Enter the one with the lowest p-value: 'HS Grad'
forward2 <- update(forward1, . ~ . + `HS Grad`)
tidy(forward2)
# Step 3: Enter the one with the lowest p-value in the rest 
fit1 <- update(forward2, . ~ . +Population, data = life_exp)
tidy(fit1)
fit2 <- update(forward2, . ~ . +Income, data = life_exp)
tidy(fit2)
fit3 <- update(forward2, . ~ . +Illiteracy, data = life_exp)
tidy(fit3)
fit4 <- update(forward2, . ~ . +Frost, data = life_exp)
tidy(fit4)
fit5 <- update(forward2, . ~ . +Area, data = life_exp)
tidy(fit5)
# Enter the one with the lowest p-value: Frost
forward3 <- update(forward2, . ~ . + Frost)
tidy(forward3)
# Step 4: Enter the one with the lowest p-value in the rest 
fit1 <- update(forward3, . ~ . +Population, data = life_exp)
tidy(fit1)
fit2 <- update(forward3, . ~ . +Income, data = life_exp)
tidy(fit2)
fit3 <- update(forward3, . ~ . +Illiteracy, data = life_exp)
tidy(fit3)
fit4 <- update(forward3, . ~ . +Area, data = life_exp)
tidy(fit4)
# Enter the one with the lowest p-value: Population
forward4 <- update(forward3, . ~ . + Population)
tidy(forward4)
# Step 5: Enter the one with the lowest p-value in the rest 
fit1 <- update(forward4, . ~ . +Income, data = life_exp)
tidy(fit1)
fit2 <- update(forward4, . ~ . +Illiteracy, data = life_exp)
tidy(fit2)
fit3 <- update(forward4, . ~ . +Area, data = life_exp)
tidy(fit3)
# P-value of all new added variables are larger than 0.10, which means that they 
# are not significant predictor, and we stop here.
```

Therefore, using backward elimination, we selected 'Murder', 'HS Grad', 'Frost' and 'Population' as predictors. The model is given by: `Life Exp` ~ `Population` + `Murder` + `HS Grad` + `Frost`

The fitted regression line is:

`Life Exp` = 71 - 0.3`Murder` + 0.047`Hs Grad` - 0.006`Frost`+ 0.00005`Population`


###Stepwise elimination

Next, I will use stepwise elimination.

```{r}
step(origin_fit, direction = 'backward')
```

Using stepwise elimination, we also selected 'Murder', 'HS Grad', 'Frost' and 'Population' as predictors. The model is given by: 

`Life Exp` ~ `Population` + `Murder` + `HS Grad` + `Frost`

The fitted regression line is:

`Life Exp` = 71 + 0.00005`Population` - 0.3`Murder` + 0.047`Hs Grad` - 0.006`Frost`

##a)

The three automatic procedures generates the same models: 

`Life Exp` ~ `Population` + `Murder` + `HS Grad` + `Frost`

The fitted regression line is also the same:

`Life Exp` = 71 + 0.00005`Population` - 0.3`Murder` + 0.047`Hs Grad` - 0.006`Frost`

##b)

The `Population` variable is a close call. In both forward and backward elimination, if we set$\alpha_{crit} = 0.05$, the variable will not be included in the model. The automatic procedures, however, are of exploratory purpose, and we should be less stringent. Therefore, we set$\alpha_{crit} = 0.10$ and in this case, we will keep `Population` variable in the model.

##c)

In this part, I will check the correlation between "Illiteracy" and "HS Grad".

```{r}
cor(life_exp$Illiteracy, life_exp$`HS Grad`)
```

The correlation between "Illiteracy" and "HS Grad" is -0.66, which means there is a relatively strong linear relationship between the two variables. They are negetively correlated because high high school graduates proportion leads to low illiteracy rate. If we include both of them into the model, they may cause collinearty problem. Fortunately, none of my 'subsets' contains both.

#Problem3.Using criterion-base procedures to select model

```{r}
# Summary of models for each size (one model per size)
best_model = regsubsets(`Life Exp` ~ ., data = life_exp)
   (rs = summary(best_model))

# Plots of Cp and Adj-R2 as functions of parameters
par(mar = c(4,4,1,1))
par(mfrow = c(1,2))

plot(2:8, rs$cp, xlab = "No of parameters", ylab = "Cp Statistic")
abline(0,1)

plot(2:8, rs$adjr2, xlab = "No of parameters", ylab = "Adj R2")

```

We can also look at it in tabular form.

```{r}
best <- function(model, ...) 
{
  subsets <- regsubsets(formula(model), model.frame(model), ...)
  subsets <- with(summary(subsets),
                  cbind(p = as.numeric(rownames(which)), which, rss, rsq, adjr2, cp, bic))
  
  return(subsets)
}  
# Select the 'best' model of all subsets for the full model
round(best(origin_fit, nbest = 1), 4)
```

From the plots, we can see that for the model with four parameters, adjusted R-squared is close to the maximum, meaning the model has a good fit of the data. In addition, from the left panel, we konw that for the model with four parameters, number of parameters is close to and less than Cp Statistic, meaning the model has low bias. The table above also shows the BIC of model with 4 parameters is only slightly greater than that of model with 5 parameters. Considering about parsimony, the model with 4 parameters is better. Therefore, using criterion-based procedures, we choose 'Murder', 'HS Grad', and 'Frost' as the best subset of predictors. The model is given by: `Life Exp` ~ `Murder` + `HS Grad` + `Frost`

#Problem4 recommendation for a final model and doing model diagnostics

The model built in part 2 is given by:

Model_L = `Life Exp` ~ `Population` + `Murder` + `HS Grad` + `Frost`

The model built in part 3 is given by:

Model_S = `Life Exp` ~ `Murder` + `HS Grad` + `Frost`

Model_S is nested in Model_L, so we use ANOVA to choose one from them.

```{r}
Model_S = lm(`Life Exp` ~ Murder + `HS Grad` + Frost, data = life_exp)
Model_L = lm(`Life Exp` ~ Population + Murder + `HS Grad` + Frost, data = life_exp)
anova(Model_S, Model_L)
```

In the partial F-test, the p-value = 0.052 > 0.05. Then, we fail to reject the null and conclude that the larger model is not superior. Therefore, we recommend the model built in part 3, which is:

Model_S = `Life Exp` ~ `Murder` + `HS Grad` + `Frost`

To get the fitted regression line:

```{r}
summary(Model_S)
```

Then, the fitted regression line is:

`Life Exp` = 71.04 -0.28`Murder` + 0.05`HS Grad` - 0.007`Frost`

##a)

```{r}
influence.measures(Model_S)
```

The result shows that the leverage values(hat values) for all observations are less than 0.3, i.e., there's no observation with high leverage value. In addition, none of them has a Cook's distance greater than 0.5, so there's no influential points.

##b)

Next, let's look at diagnostic plots.

```{r}
par(mfrow = c(2,2))
plot(Model_S)
```

As the Residual vs Fitted plot shows, residuals forms a horizontal linear band around 0, they are equally spread throughout the range of fitted values, and there is a random pattern. Also, from the scale-location plot, we can see a horizontal line with equally spread points. These facts show that residuals have constant variance. In the QQplot, we can see an overall nice line shape. Although there are some deviations on tails, which means the model is not doing a good job capturing the ends of the distribution, the model is still acceptible. Therefore, the normality assumption of the residuals holds true.

#Probelm5 Testing predictive ability

##a) 10-fold cross-validation

```{r}
set.seed(1)

# Use 10-fold validation and create the training sets
data_train <- trainControl(method = "cv", number = 10)

# Fit the 3-variables model that we chose in previous part
model_cv <- train(`Life Exp` ~ Murder + `HS Grad` + Frost,
                   data = life_exp,
                   trControl = data_train,
                   method = 'lm',
                   na.action = na.pass)

model_cv$resample

mean((model_cv$resample$RMSE))
```

```{r}
pred = predict(Model_S) %>% as.tibble()
resid = residuals(Model_S) %>% as.tibble()


boot_mse = function(df, residual, predict){
boot_resid = sample_frac(residual, replace = TRUE) #randomly resample the residuals (with replacement)
y_star = predict + boot_resid %>% as.tibble()
life_exp_new = cbind(y_star, select(df, -`Life Exp`))
model_new = lm(value ~ Murder + `HS Grad` + Frost, data = life_exp_new)
sm = summary(model_new)
mse_new = mean(sm$residuals^2) #get MSE for the new model
mse_new
}

boot_mse_10 = data_frame(
  strap_number = 1:10,
  strap_mse = rerun(10, boot_mse(life_exp, resid, pred))
) %>% unnest()

summary(boot_mse_10$strap_mse)
sd(boot_mse_10$strap_mse)

boot_mse_1000 = data_frame(
  strap_number = 1:1000,
  strap_mse = rerun(1000, boot_mse(life_exp, resid, pred))
) %>% unnest()

summary(boot_mse_1000$strap_mse)
sd(boot_mse_1000$strap_mse)
```

